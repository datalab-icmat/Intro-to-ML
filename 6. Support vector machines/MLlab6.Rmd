---
title: "MLlab-6. SVMs"
author: "DataLab"
date: " "
output: word_document
---


# Introducción

En este lab realizamos varios ejercicios en relación con 
máquinas de vector soporte.


# Clasificador de vector soporte

Este primer ejemplo adapta un ejercicio de ISLR.
Emplearemos un SVC para separar dos grupos. Generamos 
primero los datos. Primero 20 coordenadas (x,y) normales,
luego las etiquetas -1 y 1. Luego a las del grupo 1 se les
añade 1 a la coordenada x para separar un poco.
En el último comando representamos los dos grupos
para ver si son linealmente separables.
```{r,eval=FALSE}
set.seed(1)
x=matrix(rnorm(20*2), ncol=2)
z=x
y=c(rep(-1,10), rep(1,10))
y
x[y==1,]=x[y==1,] + 1
help(colors)
# estas tres juntas
par(mfrow=c(1,2))
plot(z,col=(3-y),pch=19)
plot(x,col=(3-y),pch=19)
```
Empleamos un SVC a partir de la librería e1071.
En caso de no haberlo hecho, debes descargarla. 
Tenéis manual en <https://cran.r-project.org/web/packages/e1071/e1071.pdf>.
  Creamos primero un data frame con los
datos y las etiquetas, que deben ir como factores. 
```{r,eval=FALSE}
library(e1071)
dat=data.frame(x=x, y=as.factor(y))
dat
```
Usamos la función svm. Como el kernel que ponemos es lineal,
estamos empleando en efecto SVC. La sintaxis es similar a la de otros modelos.
Cost es el coste de violación de las restricciones (por defecto es 1);
ojo la interpretación es diferente a la de clase; os la 
he puesto en el campus virtual en el doc de la libreria LIBSVM
 Por defecto las variables se
escalan. Pedimos que no se haga. Ajustamos y vemos resultados.
Plot muestra el gráfico. Los vectores soporte aparecen como cruces (x). 
svmfir$index nos los identifica como tales.
```{r,eval=FALSE}
help(svm)
svmfit=svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
```
Los gráficos por defecto son manifiestamente mejorables y puedes ver cómo se hace en
<https://www.datacamp.com/community/tutorials/support-vector-machines-r>
(así como cómo recuperar los coeficientes del modelo).
Repetimos con varios costes. Obtenemos un número mayor de vectores soporte 
cuando el coste es menor (OJO recordad comentario de antes).
```{r,eval=FALSE}
svmfit0=svm(y~., data=dat, kernel="linear", cost=0.0001,scale=FALSE)
svmfit0$index
svmfit1=svm(y~., data=dat, kernel="linear", cost=0.1,scale=FALSE)
svmfit1$index
svmfit2=svm(y~., data=dat, kernel="linear", cost=10000,scale=FALSE)
svmfit2$index
# Todos estos juntos
par(mfrow=c(2,2))
plot(svmfit0, dat)
plot(svmfit1, dat)
plot(svmfit, dat)
plot(svmfit2, dat)
```
Ahora usamos tune para hacer CV. Por defecto usa 10 pliegues.
Se pueba con varios costes (especificado en ranges). Se muestran los resultados y se elige 
el mejor valor con bestmod (guardado por tune en best.model)
```{r,eval=FALSE}
set.seed(1)
help(tune)
tune.out=tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
bestmod=tune.out$best.model
summary(bestmod)
```
Generamos un conjunto de test y empleamos predict para evaluar 
tal mejor modelo. Mostramos la matriz de confusión.
```{r,eval=FALSE}
xtest=matrix(rnorm(20*2), ncol=2)
ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,] + 1
testdat=data.frame(x=xtest, y=as.factor(ytest))
ypred=predict(bestmod,testdat)
table(predict=ypred, truth=testdat$y)
```
Repetimos  con cost=0.01.
```{r,eval=FALSE}
svmfit=svm(y~., data=dat, kernel="linear", cost=.01,scale=FALSE)
ypred=predict(svmfit,testdat)
table(predict=ypred, truth=testdat$y)
```
Finalmente consideramos un problema separable linealmente, modificando
los datos anteriores.
```{r,eval=FALSE}
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=25)
```
SVC nos debería encontrar un hiperplano separador.
Hacemos el análisis, con dos niveles de coste (uno grande y uno pequeño, pero 
recuerda la interpretación de coste). Con grande no se cometen errores,
hay tres vectores soporte pero el margen es pequeño (sobreajuste...).
Con coste 1 se cometen errores pero el modelo será algo más robusto. 
```{r,eval=FALSE}
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~., data=dat, kernel="linear", cost=1e5)
summary(svmfit)
plot(svmfit, dat)
svmfit=svm(y~., data=dat, kernel="linear", cost=1)
summary(svmfit)
plot(svmfit,dat)
```

# Máquinas de vector soporte

De nuevo adaptamos un ejercicio de ISLR. Consideramos 
SVMs (SVCs con kernel no lineales).
Creamos primero el conjunto de datos. Problema de clasificación
complicadillo... Discute cómo montamos el conjunto
```{r,eval=FALSE}
set.seed(1)
x=matrix(rnorm(200*2), ncol=2)
x
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
y
dat=data.frame(x=x,y=as.factor(y))
dat
plot(x, col=y,pch=19)
```
Tenemos que usar SVM (como antes pero con kernel no lineal,
radial en este ejemplo. Hacemos help(svm)). Usamos kernel 
radial con $\gamma =1$, partiendo en conjunto test y train.
```{r,eval=FALSE}
help(svm)
train=sample(200,100)
svmfit=svm(y~., data=dat[train,], kernel="radial",  gamma=1, cost=1)
plot(svmfit, dat[train,])
summary(svmfit)
```
Repetimos con cost mayor. Reducimos los errores de train...
pero a riesgo de sobreajustar.
```{r,eval=FALSE}
svmfit=svm(y~., data=dat[train,], kernel="radial",gamma=1,cost=1e5)
plot(svmfit,dat[train,])
```
Repetimos con gamma mayor, acentuando la 'localidad' del clasificador.
```{r,eval=FALSE}
svmfit=svm(y~., data=dat[train,], kernel="radial",gamma=10,cost=1)
plot(svmfit,dat[train,])
```
Hacemos CV con tune jugando con rangos para cost y gamma.
Ponemos la tabla de confusión para el mejor modelo.
```{r,eval=FALSE}
set.seed(1)
tune.out=tune(svm, y~., data=dat[train,], kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
matrizcon<-table(true=dat[-train,"y"], pred=predict(tune.out$best.model,newx=dat[-train,]))
matrizcon
(matrizcon[1,1]+matrizcon[2,2])/(matrizcon[1,1]+matrizcon[2,2]+matrizcon[2,1]+matrizcon[1,2])
```


# SVM con clases múltiples 

En caso de que haya más de una clase, svm utiliza el sistema uno contra 
uno. Volvemos a hacer help(svm) para recordarlo y vemos el doc de libsvm.
Generamos el conjunto de datos.
```{r,eval=FALSE}
help(svm)
set.seed(1)
help(rbind)
x
x=rbind(x, matrix(rnorm(50*2), ncol=2))
x
y
y=c(y, rep(0,50))
y
x[y==0,2]=x[y==0,2]+2
dat=data.frame(x=x, y=as.factor(y))
par(mfrow=c(1,1))
plot(x,col=(y+1))
```
Finalmente, ajustamos la SVM
```{r,eval=FALSE}
svmfit=svm(y~., data=dat, kernel="radial", cost=10, gamma=1)
plot(svmfit, dat)
```

# Predicción de supervivencia en Titanic con SVMs

Volvemos a intentar predecir la supervivencia de las víctimas del Titanic a partir de las siguientes variables:

  * survival: Supervivencia (0 = No; 1 = Si)
  * pclass: Clase de pasajero (1, 2, 3)
  * name: Nombre
  * sex: Sexo
  * age: Edad
  * sibsp: Número de hermanos/esposos/as a bordo.
  * parch: Número de padres/hijos a bordo
  * ticket: Número de ticket
  * fare: Coste del billete
  * cabin: Cabina
  * embarked: Puerto de embarque

Con el conjunto de datos anterior titanic.csv que cargamos. Adapta un ejercicio
de Alberto Torres en ICMAT.


```{r,eval=FALSE}
help(read.csv2)
data <- read.csv2('titanic.csv', na.strings = "")
fix(data)
```
Hacemos el mismo tratamiento exploratorio que en otras ocasiones.
 Vemos los valores missing y eliminamos cabin. Eliminamos name y ticket,
 que no parecen relevantes. Convertimos survived a factor. Imputamos
 age con la mediana. Quitamos ya los pocos casos con missing.
```{r,eval=FALSE}
colSums(is.na(data))
library(dplyr)
data <- select(data, -name, -ticket, -cabin)
data
data$survived <- as.factor(data$survived)
data
hist(data$age)
data$age[is.na(data$age)] <- median(data$age, na.rm = TRUE)
colSums(is.na(data))
data <- na.omit(data)
```
Ya estamos con datos limpios y separamos en 80% entrenamiento y 20% test, aleatoriamente.
Comprobamos después la similitud de ambos conjuntos.
```{r,eval=FALSE}
set.seed(1234)
idx <- sample(nrow(data), replace = FALSE, size = floor(nrow(data)*0.8))
idx
train <- data[ idx, ]
test  <- data[-idx, ]
table(train$survived)
table(test$survived)
```

Ajustamos la SVM con los parámetros por defecto. Ampliamos el tamaño
de memoria cache utilizable (lee el documento libsvm). Calculamos después la
matriz de confusión. Usamos caret para ello.
```{r,eval=FALSE}
library(e1071)
help(svm)
svm_model <- svm(survived ~ ., 
                 data = train, 
                 cachesize = 200)
summary(svm_model)
library(caret)
svm_pred <- predict(svm_model, test)
help(confusionMatrix)
confusionMatrix(svm_pred, test$survived, mode = "prec_recall", positive="1")
```

Buscamos ahora parámetros óptimos usando validación cruzada con búsqueda exhaustiva
en cost y gamma, con 5 pliegues. Mostramos los mejores parámetros. Ojo esto es 
bastante intensivo por eso pongo 9 combinaciones sólo.

```{r,eval=FALSE}
help(tune.svm)
tc <- tune.control(sampling = "cross", cross = 5)
cv <- tune.svm(survived ~ ., 
               data = train,
               tunecontrol = tc,
               cost=c(0.1,10,1000),gamma=c(0.5,2,4),
                              cache = 200)
summary(cv)
cv$best.parameters
pred_best <- predict(cv$best.model, test)
confusionMatrix(pred_best, test$survived, mode = "prec_recall", positive="1")
```

# Distintos modelos de regresión, incluyendo SVM para regresión, con los datos prostate


Adaptamos lab de Alberto Torres.
Cargamos los datos prostate y separamos train/test según los 
valores de la columna 'train'. 
```{r,eval=FALSE}
data <- read.csv('prostate.data', header = TRUE, row.names = 1)
fix(data)
head(data)
train_col <- which(colnames(data) == "train")
target_col <- which(colnames(data) == "lpsa")
train_set <- data[ data$train, -train_col]
test_set  <- data[!data$train, -train_col]
```

Ahora escalamos las variables para que tengan media 0 y varianza 1 (menos `lpsa`)

```{r,eval=FALSE}
Xtrain <- scale(train_set[, -target_col])
center <- attr(Xtrain, "scaled:center")
scale  <- attr(Xtrain, "scaled:scale")

Xtest <- scale(test_set[, -target_col], center = center, scale = scale)

train <- data.frame(Xtrain, lpsa=train_set[, target_col])
test <- data.frame(Xtest, lpsa=test_set[, target_col])
head(Xtest)
```

Ajustamos diversos modelos de regresión sobre `lpsa`.
Empleamos ModelMetrics para evaluar modelos
<https://cran.r-project.org/web/packages/ModelMetrics/ModelMetrics.pdf>
    
  * Regresión lineal
  
```{r,eval=FALSE}
library(ModelMetrics)
lm_fit <- lm(lpsa ~ ., data = train)
lm_pred <- predict(lm_fit, newdata = test)
lm_mse <- mse(test[, target_col], lm_pred)
lm_coef <- coef(lm_fit)

lm_mse
```

   * Ridge regression
```{r,eval=FALSE}  
library(ridge)

rr_fit <- linearRidge(lpsa ~ ., data = train)
rr_pred <- predict(rr_fit, newdata = test)
rr_mse <- mse(test[, target_col], rr_pred)
rr_coef <- coef(rr_fit)

rr_mse
```
  
  * Lasso
  
```{r,eval=FALSE}
library(glmnet)
library(glmnetUtils)

la_fit <- cv.glmnet(lpsa ~ ., data = train, alpha = 1)
la_pred <- predict(la_fit, newdata = test, s = "lambda.min")
la_mse <- mse(test[, target_col], la_pred)
la_coef <- coef(la_fit)[,1]
la_mse
```

  * Elastic Net
  
```{r,eval=FALSE}
library(glmnet)
library(glmnetUtils)

en_fit <- cv.glmnet(lpsa ~ ., data = train, alpha = 0.5)
en_pred <- predict(en_fit, newdata = test, s = "lambda.min")
en_mse <- mse(test[, target_col], en_pred)
en_coef <- coef(en_fit)[,1]
en_mse
```

  * PLS
  
```{r,eval=FALSE}
library(pls)

pls_fit <- plsr(formula = lpsa ~ ., data = train, validation = "CV")
ncomp_opt <- pls_fit$ncomp
pls_pred <- predict(pls_fit, newdata = test)
pls_mse <- mse(test[, target_col], pls_pred)
intercept <- pls_fit$Ymeans - pls_fit$Xmeans %*% coef(pls_fit)
pls_coef <- c(Intercept = intercept, coef(pls_fit))
pls_mse
```
  
  * SVR. Entenderás de forma sencilla SVR viendo la formulación del problema en la sección 2.4 del documento sobre libsvm
  
```{r,eval=FALSE}
library(e1071)

cv <- tune.svm(lpsa ~ ., 
               data = train, 
                cost=c(0.1,10,1000),gamma=c(0.5,2,4))
   #                           epsilon = 2^seq(-8, -1, 1))

svr_pred <- predict(cv$best.model, test)
svr_mse <- mse(test[, target_col], svr_pred)
```

 Comparamos los valores de los coeficientes de cada modelo
 y las variables que selecciona cada modelo

```{r,eval=FALSE}
coef_df <- data.frame(
  LS = round(lm_coef, 3),
   Ridge = round(rr_coef, 3),
  Lasso = round(la_coef, 3),
  ENet = round(en_coef, 3),
  PLS = round(pls_coef, 3)
)

coef_df
```
 Comparamos los errores sobre el conjunto de test

```{r,eval=FALSE}
mse_list = c(LS = lm_mse, 
              Ridge = rr_mse,
             Lasso = la_mse,
             ENet = en_mse,
             PLS = pls_mse,
             SVR = svr_mse)

mse_list
```
