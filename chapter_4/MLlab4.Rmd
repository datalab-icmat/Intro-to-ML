---
title: "MCIBLab4. Arboles"
author: "DataLab CSIC"
date: " "
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducción

En este lab revisamos conceptos sobre árboles. 
Primero hacemos ejemplos de árboles de clasificación y regresión.
Después uno de random forest, finalmente uno de gradient boosting.


# Ajuste de árboles de clasificación 

Adaptamos un ejemplo de árbol de clasificación de ISLR.
Usamos el paquete tree para los ajustes e ISLR para los datos.
Empleamos el conjutno de datos Carseats. Simplemente recordamos su estructura
pues lo usamos en un lab anterior.
Tal vez tengas que instalar el paquete tree

```{r,eval=FALSE }
library(tree)
library(ISLR)
attach(Carseats)
help(Carseats)
fix(Carseats)
```
*Sales* mide los miles de unidades vendidas y la separamos en función
de si vendieron más o menos de 8000 unidades. Predecimos 
las ventas (altas, bajas) en función de las otras variables,
salvo Sales!!. Creamos el data.frame. Ajustamos el árbol.
Lo desplegamos y comentamos el resultado. Como check, primero lo hacemos
incluyendo Sales
```{r,eval=FALSE }
High=factor(ifelse(Sales<=8,"No","Yes"))
High
Carseats=data.frame(Carseats,High)
head(Carseats)
help(tree)
tree.carseats0=tree(High~.,Carseats)
summary(tree.carseats0)
# las dos siguientes ejecútalas juntas. La primera dibuja, 
# la segunda pone etiquetas
plot(tree.carseats0)
text(tree.carseats0,pretty=0)
tree.carseats0
```
Ahora lo hacemos ya sin sales, sin repetir las operaciones
de crear data.frame etc.. ya realizadas. Discute los resultados de 
summary. Discute las variables más importantes en la clasificación
```{r,eval=FALSE }
tree.carseats=tree(High~.-Sales,Carseats)
summary(tree.carseats)
# las dos siguientes ejecútalas juntas. 
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
```
Hacemos ahora la evaluación a partir de 
un conjunto de entrenamiento y un conjunto de test. Formamos los conjuntos
entrenamos en el conjunto train. Hacemos predicción.
Creamos matriz confusión y estimamos la exactitud (accuracy).
```{r,eval=FALSE }
set.seed(2)
train=sample(1:nrow(Carseats), 200)
train
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
tree.pred
High.test
help(table)
confu<-table(tree.pred,High.test)
confu
(confu[1,1]+confu[2,2])/200
```
Ahora procedemos a podar el árbol para ver si tenemos mejores 
resultados. Queremos uno con 15 nodos terminales. Usamos la
función prune.misclass que hace cost complexity pruning
```{r,eval=FALSE }
help(prune.misclass)
prune.carseats=prune.misclass(tree.carseats,best=15)
# Los dos siguientes van juntos
plot(prune.carseats)
text(prune.carseats,pretty=0)
#ahora evaluamos
tree.pred=predict(prune.carseats,Carseats.test,type="class")
confu<-table(tree.pred,High.test)
(confu[1,1]+confu[2,2])/200
```
Compara los resultados. Como dijimos, el mejor tamaño se podría encontrar 
mediante validación cruzada con la función cv.tree. Se ilustra en la siguiente sección.

# Ajuste de un árbol de regresión
Ajustamos ahora un árbol de  regresión a los datos Boston (de la librería MASS),
adaptando un ejemplo de ISLR.
Partimos el conjunto en dos mitades. Queremos predecir la variable
medv a partir de las otras. Recuerda el signifcado de estos datos. Qué nos 
dice summary?? Interpreta el árbol
```{r,eval=FALSE }
library(MASS)
set.seed(1)
help(Boston)
train = sample(1:nrow(Boston), nrow(Boston)/2)
train
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
# Ejecuta estas dos conjuntamente
plot(tree.boston)
text(tree.boston,pretty=0)
```
Empleamos validación cruzada para ver si la poda mejora los resultados.
Usamos la función cv.tree. Propone el árbol más complej con 7 nodos...
Imagina que, tras dibujo, deseamos considerar un árbol con 
con cinco hojas, para lo que lo podamos.
```{r,eval=FALSE }
help(cv.tree)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
help(prune.tree)
prune.boston=prune.tree(tree.boston,best=5)
# Dos siguientes conjuntas
plot(prune.boston)
text(prune.boston,pretty=0)
```
Hacemos ahora las predicciones. Usamos el árbol completo, que
era el óptimos según la CV que dijimos. Qué significa la última salida?
```{r,eval=FALSE }
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
help(abline)
# Estas dos juntas
plot(boston.test,yhat)
abline(0,1)
mean((yhat-boston.test)^2)
```

## Arboles de decisión: Titanic

En este ejercicio, entrenaremos un árbol de decisión para predecir la supervivencia de los pasajeros del Titanic. Adapta una práctica de Victor Gallego y Roi Naveiro.
Ya hemos
trabajado estos datos con glms.
Cargamos primero las librerías que necesitamos. Puede que tengas que instalar algunas de ellas!!!

readr. un sistema  rápido de lectura de datos rectangulares

dplyr. un sistema de manipulación de datos.mira su github

ggplot2. para gráficos

rpart. otro paquete para árboles. googlea rpart vs tree 

rpart.plot  plot de modelos rpart

```{r, eval=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(rpart)
library(rpart.plot)
```


Cargamos primero el conjunto de datos y lo limpiamos. Procedemos en forma
parecida al lab anterior. Exploramos los datos. Recuerda el lab anterior
donde usamos estos datos.

```{r, eval=FALSE}
data <- read.csv("titanic.csv")
dim(data)
summary(data)
head(data)
data$Age
```

Imputa los NAs de variables continuas con la mediana. Convertimos las variables categóricas a variables tipo factor.

```{r, eval=FALSE}
data$Age <- sapply(data$Age, FUN=function(x) {ifelse(is.na(x),median(data$Age, na.rm = TRUE),x)})
factor_vars <- c('PassengerId','Pclass','Sex','Embarked','Cabin')
data[factor_vars] <- lapply(data[factor_vars], function(x) as.factor(x))
head(data)
```
Dividimos los datos en train y test, utilizando porcentajes 80, 20; respectivamente.
y entrenamos el árbol de decisión solo con las variables Embarked, Cabin, Sex y Pclass.
Entrenamos el árbol.
```{r, eval=FALSE}
totrain = floor( nrow(data)*0.8  )
ind_train = sample(seq_len(nrow(data)), size = totrain)
ind_train
train = data[ind_train, ]
test = data[-ind_train, ]
help(rpart)
fit <- rpart(Survived ~ Pclass + Sex + Embarked + Cabin, train, method = "class", cp=0)
```
Vemos la exactitud en el conjunto test y dibujamos el 
árbol con rpart.plot. 
```{r,eval=FALSE}
preds = predict(fit, test, type = "class")
sum(preds == test$Survived)/nrow(test)
help("rpart.plot")
rpart.plot(fit, type=1, extra = 102)
```

Podamos el árbol usando el mejor valor de cp (complexity parameter) obtenido usando validación cruzada y pintamos el árbol. Vemos la precisión.

```{r,eval=FALSE}
help(prune)
pfit<- prune(fit, cp=   fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
rpart.plot(pfit, type=1, extra = 102)
preds = predict(pfit, test, type = "class")
sum(preds == test$Survived)/nrow(test)
```

# Clasificación con Random Forest 

En este lab, entrenaremos un clasificador Random Forest sobre datos bancarios para la detección de fraude. Los datos de fraude son altamente desequilibrados.
Adaptamos un lab de Victor Gallego y Roi Naveiro. 

Cargamos primero las librerías que necesitamos. En su caso tendrás que instalarlas
antes.

randomForest. Entrenamiento de árboles aleatorios

reshape2. Paquete para transformar datos. Mira <https://rpubs.com/enralvar/ReShape2>

```{r,eval=FALSE}
library(randomForest)
library(reshape2)
library(ggplot2)
```

Descomprime los datos y carga los
conjuntos de entrenamiento y test.
Exploramos los datos. Ponemos en CV un diccionario para los 
datos. Vemos por ejemplo 
la prevalencia de fraude en cada conjunto.

```{r, eval=FALSE}
train = read.csv("train_fraud.csv")
head(train)
dim(train)
test = read.csv("test_fraud.csv")
dim(test)
sum(train$FRAUDE)/nrow(train)
sum(test$FRAUDE)/nrow(test)
```
Elimina las variables "X", "ID_TARJETA", "FC_AUTORIZACION" y
convertimos "FRAUDE" a factor
```{r, eval=FALSE}
train[, c("X", "ID_TARJETA", "FC_AUTORIZACION")] = NULL
head(train)
train$FRAUDE = as.factor(train$FRAUDE)
head(train)
dim(train)
head(train)
```

Entrenamos  un random forest con 100 árboles.
Lee antes help(randomForest). Fijate en los parámetros
ntree,mtry.
(Puede tardar varios minutos...)

```{r, eval=FALSE}
help(randomForest)
rf = randomForest(FRAUDE ~ ., data = train, ntree = 100)
rf
```

Predecimos después la probabilidad de fraude sobre el conjunto de test.
Para ello creamos un dataframe con dos columnas, prob_fraude y fraude, 
que contengan la probabilidad de fraude y la etiqueta real respectivamente.
Ordenamos el dataframe en orden decreciente de probabilidad de fraude.

```{r, eval=FALSE}
vars = colnames(train)
to_test = test[, vars]
pred_proba = predict(rf, to_test, type = "prob")
pred_proba = pred_proba[,2]
y_true = test$FRAUDE
results = data.frame("prob_fraude" = pred_proba, "fraude" = y_true)
results
res_sort = results[order(-results$prob_fraude),]
res_sort
```

A continuación damos un conjunto de funciones auxiliares para calcular
precisión y recall a una profundidad dada, así como para pintar la curva
precisión-recall.

```{r, eval=FALSE}
precision = function(df, depth){
  tot = dim(df)[1]
  inspect = floor(depth*tot)
  return(sum(df[1:inspect,2] == 1)/inspect)
}

recall = function(df, depth){
  tot = dim(df)[1]
  totF = sum(df[,2] == 1)
  inspect = floor(depth*tot)
  return(sum(df[1:inspect,2] == 1)/totF)
}


precRecallPlot = function(df, min = 0.0001, max = 0.01, step = 0.00001){
  Recall = c()
  Precision = c()
  Depth = c()
  for(i in seq(min, max, step)){
    Depth = c(Depth, i)
    Recall = c(Recall, recall(df, i))
    Precision = c(Precision, precision(df, i))
  }
  
  results = data.frame(Depth = Depth, recall = Recall, precision = Precision)
  meltedResults = melt(results, id = "Depth")
  p = ggplot(meltedResults, aes(x = Depth, y = value, color = variable))
  p = p + geom_line() + xlab("Depth") + ylab("Value")
  p
}
```

Dibujamos la curva para los resultados obtenidos

```{r,eval=FALSE}
precRecallPlot(res_sort)
```


Si se decide inspeccionar una de cada mil transacciones, ¿qué porcentaje del total de fraude se caza?, ¿qué porcentaje de veces nos equivocamos?

```{r,eval=FALSE}
recall(res_sort, 0.001)
precision(res_sort, 0.001)
```

Calculamos la importancia de las variables usando dos métodos. Utiliza la funcion varImpPlot. Tendrás que reentrenar el bosque, fijando importance = True.
Hacemos help(randomForest)para ver los dos métodos.
Hazlo con un subset pequeño del train.

```{r,eval=FALSE}
train_ind <- sample(seq_len(nrow(train)), size = 50000)
def_train = train[train_ind, ]
rf = randomForest(FRAUDE ~ ., data = def_train, ntree = 50, importance = TRUE)
```

Existe mucha discrepancia entre ambos métodos?
```{r,eval=FALSE}
help("varImpPlot")
varImpPlot(rf, type=2)
varImpPlot(rf, type=1)
```

## Boosting

En este lab, adaptado de Gallego y Naveiro, hacemos gradient boosting con los
datos de próstata.
Cargamos primero la librería gbm y el conjunto de datos. Quizá tengas que instalarla.
Los datos se describen en bastantes sitios. Aquí tenéis uno
<https://rafalab.github.io/pages/649/prostate.html>

```{r,eval=FALSE}
library(gbm)
data <- read.csv('prostate.data')
data
head(data)
dim(data)
```

Separamos en train/test de acuerdo con los valores de la columna `train`.
Primero identificamos la columna que separa train, test. P

```{r, eval=FALSE}
train_col <- which(colnames(data) == "train")
train_col
train_set <- data[ data$train, -train_col]
train_set
test_set  <- data[!data$train, -train_col]
```
Identificamos la variable a predecir lpsa.
Escalamos las variables para que tengan media 0 y varianza 1 (menos `lpsa`)
```{r,eval=FALSE}
target_col <- which(colnames(data) == "lpsa")
target_col
help(scale)
Xtrain <- scale(train_set[, -target_col])
Xtrain
center <- attr(Xtrain, "scaled:center")
scale  <- attr(Xtrain, "scaled:scale")
Xtest <- scale(test_set[, -target_col], center = center, scale = scale)
```
Creamos data.frames con escalado (el que vamos a usar)
```{r,eval=FALSE}
# Con escalado
train <- data.frame(Xtrain, lpsa=train_set[, target_col])
test <- data.frame(Xtest, lpsa=test_set[, target_col])
```
o  sin escalado. Me lo salto en la demo!!!!
```{r,eval=FALSE}
# Sin escalado
train <- data.frame(train_set[, -target_col], lpsa=train_set[, target_col])
test <- data.frame(test_set[, -target_col], lpsa=test_set[, target_col])
```

 Ajustamos un modelo para predecir el lpsa en función del resto de variables,
 para 5000 iteraciones y 4 interacciones máximas en cada árbol, y shrinkage de 0.4
 Escogemos el argumento distribution adecuado. Además, 
 obtenemos la importancia de cada variable usando summary.
```{r,eval=FALSE}
help(gbm)
model <- gbm(lpsa ~ .,data=train, distribution="gaussian",n.trees=5000, shrinkage = 0.5, interaction.depth=4)
summary(model)
```

Representamos el RMSE sobre el conjunto de test en función del número de árboles utilizados
en la predicción 
```{r,eval=FALSE}
num_iters <- seq(100,5000,100)
yhat.boost <- predict(model,newdata = test, n.trees=num_iters)
rmses <- sqrt(colMeans((yhat.boost - test$lpsa)^2))
plot(num_iters, rmses, typ='l')
min(rmses)
```

Ahora, empleamos la librería caret para ajustar los siguientes hiperparámetros mediante CV.
Quizá tengas que instalarla. 
* Usa trainControl para especificar validación cruzada con 10 pliegues
* Usa expand.grid para especificar los siguientes hiperparámetros:
  + profundidades: de 1 a 4.
  + número de iteraciones: de 50 a 1500 contando de 50 en 50.
  + shrinkage: 0.1 o 0.3.
  + n.minobsinnode: 5.
```{r, eval=FALSE}
library(caret)
help("trainControl")
fitControl <- trainControl(## 10-fold CV
                           method = "cv",
                           number = 10)
gbmGrid <-  expand.grid(interaction.depth = c(1, 2, 3), 
                        n.trees = (1:30)*50, 
                        shrinkage = c(0.5,0.01),
                        n.minobsinnode = 5)
model2 <- train(lpsa ~ ., data = train, 
                 method = "gbm", 
                 trControl = fitControl,
                 tuneGrid = gbmGrid,
                 verbose = FALSE)
```

Mostramos los hiperparámetros del mejor modelo y hacemos plot del objeto modelo ajustado
```{r,eval=FALSE}
print(model2$bestTune)
plot(model2)
```

Ajustamos gbm al training set entero con los hiperparámetros anteriores. Realizamos  predicciones sobre el conjunto inicial de test y obtener el nuevo RMSE
```{r,eval=FALSE}
model_opt <- gbm(lpsa ~ .,data=train, distribution="gaussian",
                 n.trees=model2$bestTune$n.trees,
                 interaction.depth=model2$bestTune$interaction.depth, 
                 shrinkage = model2$bestTune$shrinkage)

yhat2 <- predict(model_opt,newdata = test, n.trees = model2$bestTune$n.trees)
rmse <- sqrt(mean((yhat2 - test$lpsa)^2))
print(rmse)
```

