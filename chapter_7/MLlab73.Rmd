---
title: "MLlab-7-3. Aprendizaje profundo para sucesiones
Intro"
author: "DataLab CSIC"
date: " "
output: word_document
---


# Introducción

En este lab realizamos varios ejercicios en relación con 
con aprendizaje profundo y procesamiento de sucesiones 
con ayuda de Keras. Hacemos versiones CPU. 
Completaremos con algunas versiones con Google Collab.


# Recordatorio. Ojo carga keras 

Recuerda que hicimos ya varios ejemplos con Keras.
Por si no lo hiciste, recuerda que has de cargarlo.

Copiamos lo que dijimos entonces. Echamos un ojito en  <https://blog.rstudio.com/2017/09/05/keras-for-r/> y 
<https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/>. Más breve con 
help(Keras). Esencialmente
tenemos que instalarla el paquete Keras R, cargar el núcleo de keras y luego
con install_keras() cargamos también TensorFlow (esto se hace una vez, reinicializa
RStudio cuando lo hayas hecho lo comentas). Si has instalado ya Keras, bastante
con que la cargues.
```{r, eval=FALSE}
library(keras)
help(keras)
#install_keras()
```

Vamos ya al tema que nos ocupa. Adaptamos varias prácticas de Alberto Torres, Victor Gallego
y Roi Naveiro.

## LSTM para series temporales

Comenzamos realizando predicciones con una serie típica en economía (tasas de interés
mensual en USA) con ayuda 
de un modelo LSTM. Un poco overkilling; tenéis ARIMAS, DLMs y demás para este propósito.
Si te animas ajusta alguno de los modelos que conozcas.

 Cargamos los datos y representamos la serie temporal.
 As usual, pon el path via session, set working directory etc...

```{r, eval=FALSE}
df <- read.csv('data_interest.csv')
fix(df)
dim(df)
# Nos interesa predecir la variable Value
# Mostramos las primeras  observaciones y dibujamos la serie temporal y su acf
Series <- df$Value
head(Series)
plot(Series, typ='l',main="Datos", 
     xlab="Mes", ylab="Tasa de interés USA")
acf(Series)
```

Diferenciamos la series para reducir la autocorrelación (ay, si usásemos
DLMs....)


```{r, eval=FALSE}
diffed = diff(Series, differences = 1)
head(diffed)
length(diffed)
acf(diffed)
plot(diffed, typ='l',main="Datos", 
     xlab="Mes", ylab="Dif Tasa de interés USA")

```
Ahora creamos una dataframe con dos columnas, la segunda el dato actual (x_t) y la primera el dato anterior (x_{t-1}) (retardados, though).  Después partimos los datos en
conjunto de entrenamiento (70%) y de test (30%) (ojo aquí no es aleatoria
la partición, primer 70% training, resto test). Además escalamos los datos (entre -1 y 1).
```{r, eval=FALSE}
# Definimos función para construir datos retardados 
lag_transform <- function(x, k= 1)  
  {lagged =  c(rep(NA, k), x[1:(length(x)-k)])
  DF = as.data.frame(cbind(lagged, x))
  colnames(DF) <- c( paste0('x-', k), 'x')
  DF[is.na(DF)] <- 0
  return(DF)}
# construimos datos retardados y los vemos
supervised = lag_transform(diffed, 1)
head(supervised)
fix(supervised)
# partimos los datos
N = nrow(supervised)
n = round(N *0.7, digits = 0)
train = supervised[1:n, ]
dim(train)
test  = supervised[(n+1):N,  ]
# escalamos los datos. definimos función para ello
scale_data = function(train, test, feature_range = c(0, 1)) 
  {  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = ((x - min(x) ) / (max(x) - min(x)  ))
  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))
  
  scaled_train = std_train *(fr_max -fr_min) + fr_min
  scaled_test = std_test *(fr_max -fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )}
# Y aplicamos la función
Scaled = scale_data(train, test, c(-1, 1))

# y separamos el predictor y la variable a predecir (para train y test)
y_train = Scaled$scaled_train[, 2]
x_train = Scaled$scaled_train[, 1]

y_test = Scaled$scaled_test[, 2]
x_test = Scaled$scaled_test[, 1]

# Además, dejamos definida transfo inversa para deshacer el reescalado anterior (hará falta # para mostrar las predicciones  de manera más entendible)

invert_scaling = function(scaled, scaler, feature_range = c(0, 1))
  {  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues }
  
  return(inverted_dfs) } 
```

Un último paso, convertimos el input a 3-dim para hacerlo compatible 
con la librería keras y especificamos algunos parámetros que usaremos
al entrenar la red. 
```{r, eval=FALSE}
x_train
dim(x_train) <- c(length(x_train), 1, 1)
x_train

X_shape2 = dim(x_train)[2]
X_shape2
X_shape3 = dim(x_train)[3]
X_shape3
batch_size = 1                # Tamaño de los minilotes que emplearemos
units = 1                     # Número de unidades en la lstm que emplearemos
```


Pasamos ya a definir el modelo. Una capa lstm seguida de una densa. 
Ponemos stateful = TRUE. Con help(layer_lstm) entendemos 
el modelo. Con summary entemdemos algo más del modelo.

Después compilamos.

Luego ajustamos con 50 épocas.

```{r, eval=FALSE}
# Vemos definiciones de las capas usadas (layer_lstm es nueva)
help(layer_lstm)
help(layer_dense)
# Definimos modelo
model <- keras_model_sequential() 
model%>%
  layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful= TRUE)%>%
  layer_dense(units = 1)
# Compilamos (MSE, adam, regularizador con decay en la tasa de
# aprendizaje, accuracy) y mostramos el modelo. Hacemos help para compile
# y optimizer_adam
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam( lr= 0.02, decay = 1e-6 ),  
  metrics = c('accuracy')
)
summary(model)
# Ponemos 50 épocas
Epochs = 50   
for(i in 1:Epochs ){
  model %>% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
  model %>% reset_states()
}
```

Una vez ajustado, hacemos las predicciones, deshacemos las transfos y 
representamos.

```{r, eval=FALSE}
L = length(x_test)
scaler = Scaled$scaler
scaler
predictions = numeric(L)
for(i in 1:L){
  X = x_test[i]
  dim(X) = c(1,1,1)
  yhat = model %>% predict(X, batch_size=batch_size)
  # deshacemos el reescalado
  yhat = invert_scaling(yhat, scaler,  c(-1, 1))
  # deshacemos la diferenciacion
  yhat  = yhat + Series[(n+i)]
  # guardamos
  predictions[i] <- yhat
}

# Dibujamos los valore- Los dos juntos
plot(Series, typ='l',main="Datos", 
     xlab="Mes", ylab="Tasa de interés USA")
lines(c(rep(NA,135-40), predictions), col='red')
```

Calculamos finalmente el MAE sobre el conjunto test. Usamos un benchmark como
modelo base.

```{r, eval=FALSE}

true_labels <- Series[(n+1):(n+L)]
MAE_lstm = mean(abs(predictions - true_labels))
print(MAE_lstm)


MAE_base = mean(abs(Series[(n):(n+L-1)] - true_labels  ))
print(MAE_base)
```

Un buen ejercicio es rehacer este lab con alguno de los modelos de TS que conoces. Tienes paquetes en R para hacerlo.

## Datos Oahu

Rehacemos las predicciones de los datos Oahu con modelos LSTM. Para detalles de datos 
y demás recuerda el Lab anterior. Explicamos sólo la parte referida a predicción.

Cargamos librerías. Tal vez tengáis que instalarlas, aunque ya las hemos empleado antes.

```{r, , eval=FALSE}
library(lubridate)
library(readr)
library(dplyr)
library(stringr)
# esta creo que no la hemos usado antes
help(stringr)
# library(keras)
```

Cargamos datos y seleccionamos variables como en lab anterior.

```{r, eval=FALSE}
df <- read_csv('oahu_min.csv', 
               locale = locale(tz = "Pacific/Honolulu"))
df <- df %>% 
  select(Datetime, starts_with("GH"), -GH_AP3) %>%
  rename_all(~str_remove(., "GH_"))
```

Creamos un data un dataframe con 3 lags o desplazamientos
y partimos en entrenamiento, validación y test:

  * Entrenamiento, hasta 2011-06-01
  * Validación, de 2011-06-01 hasta 2011-09-01
  * Test, a partir de 2011-09-01

Después creamos las matrices X e y, de la siguiente forma (parecido al lab+anterior):

  * X: todos los piranómetros en el tiempo t-1, t-2 y t-3
  * y: el piranómetro "GH_DH1" en el tiempo t

Y convertimos las matrices X en un array de tamaño (n_filas, n_sensores, n_lags)

```{r, eval=FALSE}
# crear data frame
df2 <- df
nlags <- 3
for (t in 1:nlags)
  {
  df1 <- mutate(df, Datetime = Datetime + minutes(t))
  df2 <- inner_join(df1, df2, by = 'Datetime', 
                    suffix = c("", paste0("_t-", t)))
}
head(df2)
dim(df2)
# partición
end_train <- as.Date("2011-06-01")
end_val   <- as.Date("2011-09-01")

help(filter)
train <- filter(df2, Datetime < end_train)
val   <- filter(df2, Datetime > end_train, Datetime < end_val)
test  <- filter(df2, Datetime > end_val)
# Creamos X e y 
X_train <- as.matrix(select(train, contains("t-")))
y_train <- as.matrix(select(train, DH1))

X_test <- as.matrix(select(test, contains("t-")))
y_test <- as.matrix(select(test, DH1))

X_val <- as.matrix(select(val, contains("t-")))
y_val <- as.matrix(select(val, DH1))

# Conversión de las matrices  
dim(X_train) <- c(nrow(X_train), ncol(X_train)/nlags, nlags)
dim(X_test)  <- c(nrow(X_test),  ncol(X_test)/nlags,  nlags)
dim(X_val)   <- c(nrow(X_val),   ncol(X_val)/nlags,   nlags)

help(aperm)
X_train <- aperm(X_train, c(1, 3, 2))
X_test  <- aperm(X_test,  c(1, 3, 2))
X_val   <- aperm(X_val,   c(1, 3, 2))
```


Definimos ya la red neuronal con 2 capas ocultas, 

  1. LSTM, 50 unidades, activación ReLU
  
  2. Densa, 128 unidades en la capa oculta, activaciones ReLU y regularización $l_2$ ($\lambda = 0.01$)
  
Y entrenamos la red durante 3 épocas usando como función de pérdida el MAE y el optimizador Adam, con tamaño de mini-batch de 128. (ponemos sólo 3 épocas por la intensidad de los 
cálculos)

```{r, eval=FALSE}
help(layer_lstm)
# Inicializa modelo
model <-  keras_model_sequential()

# Añadimos las capas 
model %>% 
     layer_lstm(units = 50, activation = 'relu', 
                input_shape = c(nlags, 16)) %>%
    layer_dense(units = 128, activation = 'relu') %>%
    layer_dense(units = 1, activation = 'linear')

summary(model)

# Definimos el entrenamiento del modelo


model %>% compile(
  loss = "mae",
  optimizer = optimizer_adam()
)

history <- model %>% fit(
  X_train, y_train, 
  epochs = 3, batch_size = 128, 
  validation_data = list(X_val, y_val),
  verbose = 1
)

plot(history)
```

Finalmente, calculamos el error en el conjunto de test.

```{r, eval=FALSE}
evaluate(model, X_test, y_test)
```

# Análisis de sentimientos con LSTM 

En esta práctica realizaremos un problema de clasificación (sentimiento positivo o negativo) sobre una base de datos de críticas de películas (a favor o en contra)

 Cargamos los paquetes necesarios y definimos parámetros
```{r, eval=FALSE}
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
# Este no lo hemos usado antes, creo 
help(purrr)

# Parametros
maxlen <- 40
```

Ejecuta el siguiente fragmento y explora train_data y train_labels. Escoge un vocabulario de 10000 palabras como input a dataset_imdb (que viene en Keras de origen)

```{r, eval=FALSE }
help("dataset_imdb")
dataset_imdb_word_index()
imdb <- dataset_imdb(num_words = 10000)
imdb

# cargamos datos y etiquetas de train y test
c(train_data, train_labels) %<-% imdb$train
c(test_data, test_labels) %<-% imdb$test

word_index <- dataset_imdb_word_index()

paste0("Training entries: ", length(train_data), ", labels: ", length(train_labels))
# Mostramos un par de reviews
train_data[[1]]
train_data[[5]]
```

Con ayuda del siguiente fragmento, podemos ver el texto original.

```{r, eval=FALSE}
word_index_df <- data.frame(
  word = names(word_index),
  idx = unlist(word_index, use.names = FALSE),
  stringsAsFactors = FALSE
)
# mostramos las palabras
word_index_df

# Los primeros indices estan reservados 
word_index_df <- word_index_df %>% mutate(idx = idx + 3)
word_index_df <- word_index_df %>%
  add_row(word = "<PAD>", idx = 0)%>%
  add_row(word = "<START>", idx = 1)%>%
  add_row(word = "<UNK>", idx = 2)%>%
  add_row(word = "<UNUSED>", idx = 3)

# ahora mostramos las palabras ordenadas
word_index_df <- word_index_df %>% arrange(idx)

# esta función ayuda a decodificar la critica
decode_review <- function(text){
  paste(map(text, function(number) word_index_df %>%
              filter(idx == number) %>%
              select(word) %>% 
              pull()),
        collapse = " ")
}

train_data[[1]]
decode_review(train_data[[1]])
train_data[[13]]
decode_review(train_data[[13]])
```

El siguiente fragmento añade padding a todas las secuencias para que tengan la misma longitud (256)

```{r, eval=FALSE}
train_data <- pad_sequences(
  train_data,
  value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
  padding = "post",
  maxlen = 256
)

test_data <- pad_sequences(
  test_data,
  value = word_index_df %>% filter(word == "<PAD>") %>% select(idx) %>% pull(),
  padding = "post",
  maxlen = 256
)

# Veamos un ejemplo para entender el efecto de padding
train_data[1, ]

```


Definmos ya el modelo. 

  - Capa de embedding: desde vocab_size a 16
  - Capa de promedio (global_average_pooling)
  - Capa densa de 16 unidades, con relu como no-linealidad
  - Capa densa a 1 unidad, con la no-linealidad adecuada para emitir una probabilidad.
  


```{r, eval=FALSE }
help("layer_embedding")
help("layer_global_average_pooling_1d")

vocab_size <- 10000

model <- keras_model_sequential()
model %>% 
  layer_embedding(input_dim = vocab_size, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% summary()
```


 Entrenamos  con Adam, y escogemos accuracy como métrica auxiliar.

```{r, eval=FALSE}
model %>% compile(
  optimizer = 'adam',
  loss = 'binary_crossentropy',
  metrics = list('accuracy')
)
```

Escoge como validación los 10000 ejemplos de train

```{r, eval=FALSE}
x_val <- train_data[1:10000, ]
partial_x_train <- train_data[10001:nrow(train_data), ]

y_val <- train_labels[1:10000]
partial_y_train <- train_labels[10001:length(train_labels)]

```

Entrenamos con 25 épocas y 512 como tamaño de batch

```{r, eval=FALSE }
history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 25,
  batch_size = 512,
  validation_data = list(x_val, y_val),
  verbose=1
)

```


Evalua los resultados en el test set

```{r, eval=FALSE}
results <- model %>% evaluate(test_data, test_labels)
results
```

Exploramos otras arquitecturas.

```{r, eval=FALSE }

vocab_size <- 10000

model <- keras_model_sequential()
model %>% 
  layer_embedding(input_dim = vocab_size, output_dim = 4) %>%
  layer_lstm(4, return_sequences = TRUE, go_backwards=TRUE) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 4, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% summary()
```

## LSTM para generación de texto

En esta práctica utilizaremos una red recurrente para generar nombres de municipios españoles.

Cargamos los paquetes necesarios y parámetros. Quizá tengas que instalar alguna de ellas.
Hacemos help(tokenizers) por su interés.

```{r, eval=FALSE}
library(keras)
library(readr)
library(stringr)
help("stringr")
library(purrr)
library(tokenizers)
help(tokenizers)
```

Cargamos el fichero munis.txt y lo convertimos en lista de caracteres, pasando mayúsculas a minúsculas.

¿Cuántos caracteres únicos hay?

```{r, eval=FALSE }
# estas dos son solo para que veamos los datos orignales y como se convierten
junk<-read_lines('munis.txt')
junk
# aquí retomamos el lab. fijaos en el pipeline del fichero al 'tokenizado' (sorry!!)
# hacemos antes help de las funciones nuevas
help("str_to_lower")
help("str_c")
help("tokenize_characters")
# al lío
text <- read_lines('munis.txt') %>%
  str_to_lower() %>%
  str_c(collapse = "\n") %>%
  tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("corpus length: %d", length(text)))
# vemos en que se ha convertido el texto
text
chars <- text %>%
  unique() %>%
  sort()
print(sprintf("total chars: %d", length(chars))) 
chars

```

El siguiente fragmento de código auxiliar construye los objetos dataset$sentence y dataset$nextchar (sentence cada maxlen caracteres, nextchar siguiente caracter)

```{r, eval=FALSE}
# Cortamos el texto en secuencias de maxlen caracteres. haz help de map y transpose cuando tengas tiempo
maxlen <- 40
dataset <- map(
  seq(1, length(text) - maxlen - 1, by = 3), 
  ~list(sentence = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
  )
dataset[1]
dataset <- transpose(dataset)
# dataset[1]

# Vectorización. Primero creamos las estructuras de datos
x <- array(0, dim = c(length(dataset$sentence), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentence), length(chars)))
# Ahora hacemos la vectorización propiamente
for(i in 1:length(dataset$sentence)){
  
  x[i,,] <- sapply(chars, function(x){
    as.integer(x == dataset$sentence[[i]])
  })
  
  y[i,] <- as.integer(chars == dataset$next_char[[i]])
  
}
```

 Creamos una RNN con una capa LSTM con 32 unidades, seguida de una capa feed-forward (proyectando a un espacio de dimensión el número de caracteres), seguida de la capa apropiada para convertir lo anterior en probabilidades.

Como optimizador, escogemos Adam con una tasa de aprendizaje de 0.01

```{r, eval=FALSE}
help("layer_activation_softmax")
model <- keras_model_sequential()
model %>%
  layer_lstm(32, input_shape = c(maxlen, length(chars))) %>%
  layer_dense(length(chars)) %>%
  layer_activation("softmax")
summary(model)
optimizer <- optimizer_adam(lr = 0.01)
model %>% compile(
  loss = "categorical_crossentropy", 
  optimizer = optimizer
)
```

El siguiente codigo a medida que entrena va generando muestras (predicciones).
Observa qué ocurre cuando cambiamos el parámetro diversity. 

```{r, eval=FALSE }

# definimos la función sample_mod para generar las predicciones con la salida más probable
sample_mod <- function(preds, temperature = 1)
  {  preds <- log(preds)/temperature
  exp_preds <- exp(preds)
  preds <- exp_preds/sum(exp(preds))
    rmultinom(1, 1, preds) %>% 
    as.integer() %>%
    which.max() }
# otra función para ir imprimiendo resultados
on_epoch_end <- function(epoch, logs) {
    cat(sprintf("epoch: %02d ---------------\n\n", epoch))
    for(diversity in c(0.2, 1))
      {
        cat(sprintf("diversity: %f ---------------\n\n", diversity))
    
    start_index <- sample(1:(length(text) - maxlen), size = 1)
    sentence <- text[start_index:(start_index + maxlen - 1)]
    generated <- ""
    
    for(i in 1:400){
      
      x <- sapply(chars, function(x){
        as.integer(x == sentence)
      })
      x <- array_reshape(x, c(1, dim(x)))
      
      preds <- predict(model, x)
      next_index <- sample_mod(preds, diversity)
      next_char <- chars[next_index]
      
      generated <- str_c(generated, next_char, collapse = "")
      sentence <- c(sentence[-1], next_char)
          }
    
    cat(generated)
    cat("\n\n")
    
  }
}

print_callback <- callback_lambda(on_epoch_end = on_epoch_end)
# aqui viene ya el ajuste (con las predicciones incluidas)
model %>% fit(
  x, y,
  batch_size = 128,
  epochs = 3,
  callbacks = print_callback
)
```



