---
title: "MLlab-8. Aprendizaje no supervisado"
author: "DataLab CSIC"
date: " "
output: word_document
---


# Introducción

En este lab hacemos ejemplos de aprendizaje no supervisado.
Ya hemos visto algunas cosas (PCAs, Autoencoders,...) antes. Aquí
completamos con ejemplos de cosas de clase. 


# RD. FMN

Adaptamos lab de Victor Gallego y Roi Naveiro. Plantearemos un problema de
factorización de una matriz no negativa que representará el número de veces que cierta persona ha votado a cierto partido político en los últimos años.

Construimos primero la matriz de datos, siendo estos los votos de cada persona.

  * Irene: 1 Podemos, 2 PSOE.
  * Pepe: 2 Podemos, 1 PSOE, 1 Ciudadanos.
  * Francisco: 6 Vox.
  * Luisa: 1 Ciudadanos, 3 PP, 2 Vox.
  * Cayetano: 4 Ciudadanos, 1 PP.
  * Eva: 2 Podemos.
  * Eugenia: 1 Podemos.
  * Mario: 1 Vox

```{r, eval=FALSE}

# Ps, PSOE, Cs, PP, Vox

irene <- c(1, 2, 0, 0, 0)
pepe <- c(2, 1, 1, 0, 0)
fran <- c(0, 0, 0, 0, 6)
luisa <- c(0, 0, 1, 3, 2)
cayetano <- c(0, 0, 4, 1, 0)
eva <- c(2, 0, 0, 0, 0)
eugenia <- c(1, 0, 0, 0, 0)
mario <- c(0, 0, 0, 0, 1)

X <- cbind(irene, pepe, fran, luisa, cayetano, eva, eugenia, mario)
rownames(X) = c("Ps", "PSOE", "Cs", "PP", "Vox")

print("Matriz de datos")

print(X)
```

 Factorizamos la matrix **X** para distintas dimensiones latentes.
 Usamos nmf del paquete NMF. Instalamos y cargamos. 
 Hemos de instalar Biobase a través de BiocMAnager. Hacemos help
 as usual en estos casos.

```{r, eval=FALSE}
BiocManager::install("Biobase")
library(NMF)
help(NMF)
help(nmf)
```
Hacemos la factorización para d=3. Interpreta los clusters.
Prueba con otros d. 
```{r, eval=FALSE}
d = 3
pol_nmf <- nmf(X, d)
fit(pol_nmf)

# Factores
print("Matrix de factores")
W <- basis(pol_nmf)
print(W)
#dim(W)

print("Matrix de coeficientes")
H <- coef(pol_nmf)
print(H)
#dim(H)
```

 
 Reconstruye la matriz y recomienda partidos a los que votar a las diferentes personas.

```{r, eval=FALSE}

print("Matrix reconstruida")
V.hat <- fitted(pol_nmf)

print(V.hat)

```

## RD. PCA vs tSNE

Hacemos ahora una comparación de PCA con tSNE. Aparecen versiones por todas partes así que no sé muy buen a quien atribuírsela. Usamos los datos iris.

rm(list = ls())

```{r, eval=FALSE}
library(ggplot2)
library(factoextra)
library(Rtsne)
mydata = iris
dim(mydata); head(mydata)
fix(mydata)
````
Hacemos PCA (escalamos datos) y representamos el screeplot para entender la historia.
```{r, eval=FALSE}
pca = prcomp(mydata[,-5],
             center = T,
             scale. = T)

screeplot(pca,type = "line")
```
Dos PCs parecen OK. Dibujamos cositas.
```{r, eval=FALSE}
data_plot <- as.data.frame(pca$x)
data_plot$source <- mydata$Species
data_plot
# preparar figura 
variance.percent_1 <- round(factoextra::get_eig(pca)$variance.percent,digits = 2)
pca.plot <- ggplot(data_plot,aes(x=PC1,y=PC2,color=source)) + 
  geom_point(aes(alpha=0.5)) +
  theme(panel.background = element_blank(),
        legend.position = "right",
        panel.border=element_rect(fill=NA),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background=element_blank(),
        axis.text.x=element_text(colour="black"),
        axis.text.y=element_text(colour="black"),
        axis.ticks=element_line(colour="black"),
        plot.margin=unit(c(1,1,1,1),"line")) +
  xlab(paste0("PC1"," (",variance.percent_1[1],"%)")) +
  ylab(paste0("PC1"," (",variance.percent_1[2],"%)")) +
  scale_alpha_continuous(guide=FALSE) +
  stat_ellipse( level = 0.95)+
  labs(title = "PCA")
pca.plot
```
Ahora hacemos algo parecido con tSNE. tsne hates datos duplicados...
```{r, eval=FALSE}
mydata.tsne = unique(iris)
str(mydata.tsne); dim(mydata.tsne)
```
Hacemos help con Rtsne. Factorizamos la variable especie.
```{r, eval=FALSE}
help(Rtsne)
mydata.tsne$Species <- as.factor(mydata.tsne$Species)
class(mydata.tsne)
# Lanzamos tsne!!
tsne = Rtsne(mydata.tsne[,-5], dims = 2, perplexity = 30, 
             verbose=TRUE, max_iter = 1000, pca = T)
class(tsne); names(tsne)
tsne    
### dibujamos quelque chose 
tsne.plot = ggplot(as.data.frame(tsne[["Y"]]), mapping = aes(x = V1, y = V2,
            col = mydata.tsne$Species))+
  geom_point()+
  labs(x = 't-SNE Dimension 1', y = 't-SNE Dimension 2', title = 't-SNE')
tsne.plot 
### algun dibujo más. todos juntos hasta el final
colors = rainbow(length(unique(mydata.tsne$Species)))
names(colors) = unique(mydata.tsne$Species)
par(mgp=c(2.5,1,0))
plot(tsne$Y, t='n', main="tSNE", xlab="tSNE dimension 1", ylab="tSNE dimension 2", "cex.main"=2, "cex.lab"=1.5)
text(tsne$Y, labels=mydata.tsne$Species, col=colors[mydata.tsne$Species])
```

Aquí hacemos otro ejemplo de tSNE en un problema de mayor dimensión que reducimos
a dimensión 2. USamos un wrapper que cubre a ggplot2 y Rtsne.

```{r, eval=FALSE}
BiocManager::install("M3C")
library(M3C)
help(M3C)
# data loading
#load(unzip("pollen_test_data.zip"))
load("pollen_test_data.RData")
pollen$data
david<-pollen$data
david
dim(david)
chema<-pollen$celltypes
chema
# lanzamos tsne 
#tsne(pollen$data,colvec=c('gold'))
tsne(pollen$data)
# ponemos mas colores
tsne(pollen$data,labels=as.factor(pollen$celltypes))
# aquí ponemos como referencia el tipo HBG1
tsne(pollen$data,
labels=scale(as.numeric(pollen$data[row.names(pollen$data)=='HBG1',])),
controlscale = TRUE,scale=2)
```

## Un ejemplo de UMAP

Consideramos el mismo problema con UMAP. Seguimos empleando el wrapper MC3, que ya está cargado.
```{r, eval=FALSE}
library(M3C)
umap(pollen$data,colvec=c('skyblue'))
# Añadimos algun color más
umap(pollen$data,labels=as.factor(pollen$celltypes),controlscale=TRUE,scale=3)
# Ponemos como referencia GAGE4
umap(pollen$data, labels=scale(as.numeric(pollen$data[row.names(pollen$data)=='GAGE4',])),
controlscale = TRUE,scale=2,legendtitle = 'GAGE4')
```

Hay muchos paquetes de R que implementan UMAP (y tSNE)...

## CL. K-means

Hacemos un caso de K-means. Cargamos las librerías necesarias. 

```{r, eval=FALSE}}
library(ggplot2)
library(dplyr)
library(ggmap)
library(lubridate)
```

Aplicamos K-means a los datos generados por UBER en la ciudad de Nueva York 
para así tratar de localizar los barrios de NYC: Queens, Bronx, Brooklyn, Manhattan y Staten Island.


Descargamos del CV, descomprimimos y exploramos los datos. Los tenemos el campus virtual.


```{r, eval=FALSE}
# Load the .csv files
apr14<-read.csv("data/uber-raw-data-apr14.csv")
head(apr14)
may14<-read.csv("data/uber-raw-data-may14.csv")
jun14<-read.csv("data/uber-raw-data-jun14.csv")
jul14<-read.csv("data/uber-raw-data-jul14.csv")
aug14<-read.csv("data/uber-raw-data-aug14.csv")
sep14<-read.csv("data/uber-raw-data-sep14.csv")
data<-bind_rows(apr14,may14,jun14,jul14,aug14,sep14)
```

Aplicamos el algoritmo K-means a los datos de longitud y latitud de cada recogida de UBER.
Exploramos la estructura del objeto devuelto.
```{r, eval=FALSE}
help(kmeans)
km <- kmeans(data[,2:3], 5)
str(km)
data$barrio <- as.factor(km$cluster)
head(data)
```

* cluster: vector de enteros que indica a que cluster pertenece cada observación.
* centers: matriz de los centroides de cada cluster.
* withinss: vector con la dispersión dentro de cada cluster.
* tot.withinss: sum(withinss).
* size: número de puntos de cada cluster

Repite la asignación de clusters 10 veces y seleccionamos el mejor clustering.

```{r, eval=FALSE}
totw = 10^7
for(i in 1:10){
  km <- kmeans(data[,2:3], 5)
  if(km$tot.withinss < totw){
    km_def = km
    totw = km$tot.withinss
  }
}
```

Seleccionamos un subconjunto de tamaño 100000 de los datos, y 
los representamos  gráficamente sobre el mapa de NYC, pintando los distintos cluster de diferente color.

```{r, eval=FALSE}
data_red = data[sample(nrow(data), 100000), ]

nyc <- get_stamenmap(bbox = c(left = -74.45, bottom = 40.35, 
                                  right = -73.65, top = 41.1), 
                         zoom = 10)

ggmap(nyc)  + geom_point(aes(x = Lon, y = Lat, colour = as.factor(barrio)), data = data_red) +
  ggtitle("Barrios de Nueva York") + ggsave("prueba.jpg")
```

Estudiamos ahora el uso de UBER por hora y barrio. 
```{r, eval=FALSE}
data$Date.Time = mdy_hms(data$Date.Time,tz=Sys.timezone())
data$Mes = month(data$Date.Time)
data$Hora = hour(data$Date.Time)
ggplot(data = data, aes(x = Hora ,colour = barrio, fill = barrio)) + geom_histogram(binwidth = 24) + facet_wrap(~barrio)
```

Finalmente, estudiamos la evolución (mensual) del uso de uber por barrio.

```{r, eval=FALSE}
data$Mes <- as.double(data$Mes)
data$Hora <- as.double(data$Hora)
data_MB <- count_(data, vars = c('Mes', 'barrio'), sort = TRUE) %>% 
  arrange(Mes, barrio)
data_MB = data.frame(data_MB)
ggplot(data = data_MB, aes(x = Mes, y=n, colour = barrio)) + geom_line()
```

## Clustering jerárquico

Utilizaremos el dataset votes.repub, consistente en una tabla con el porcentaje de votos dados al candidato republicano en elecciones presidenciales desde 1856 a 1976. La fuente  es

S. Peterson (1973): A Statistical History of the American Presidential Elections. New York: Frederick Ungar Publishing Co. Data from 1964 to 1976 is from R. M. Scammon, American Votes 12, Congressional Quarterly.


Obtenemos el dataset desde la librería cluster y los exploramos, representandolos graficamente.

```{r, eval=FALSE}
votes.repub <- cluster::votes.repub
library(ggplot2)
votes <- reshape2::melt(t(votes.repub))

votes$Var1 <- sub(pattern = "X", 
                replacement = "", x=votes$Var1)
votes$Var1 <- strtoi(votes$Var1)
colnames(votes) <- c('Year',  'State', 'VotePerc.')

ggplot(votes, aes(x=Year)) + 
  geom_line(aes(y=VotePerc., col=State)) +
  theme(legend.position = "none")
#
```

Calculamos  la matriz de distancias usando la función dist
Previamente, aplicamos la transformación angular (arcsin(sqrt(x/100))) a los datos.
Representamos la matriz de distancias utilizando la función image.


```{r , eval=FALSE}
asinTransform <- function(p) { asin(sqrt(p/100.)) }

d_votes <- dist(asinTransform(votes.repub))

dim <- nrow(votes.repub)
# Las tres siguientes juntas
image(1:dim, 1:dim, as.matrix(d_votes), axes = FALSE, xlab="", ylab="")

axis(1, 1:dim, rownames(votes.repub), cex.axis = 0.5, las=3)
axis(2, 1:dim, rownames(votes.repub), cex.axis = 0.5, las=1)
```

Realizamos  un AC  jerárquico usando link completo mediante la función hclust.
Exploramos el objeto resultante. Representamos el dendrograma usando la función plot y as.dendrogram de tal objeto. Comentamos los estados parecidos. 

```{r , eval=FALSE}
hc_votes <- hclust(d_votes, method = "complete")
str(hc_votes)
dend <- as.dendrogram(hc_votes)
plot(dend)
```


Ahora utilizaremos la librería dendextend para tener más flexibilidad con los dendogramas.
Usando la función color_branches, obtén dos clusters.

```{r , eval=FALSE}
#devtools::install_version("fpc", version = "2.1-4", repos = "http://cran.us.r-project.org")
#install.packages('dendextend')
library(dendextend)

dend <- color_branches(dend, k=2) 
plot(dend)
```

Con la librería gplots, representaremos además un heatmap para comprobar el dendograma.

```{r , eval=FALSE}
#install.packages('gplots')
library(gplots)

heatmap.2(as.matrix(votes.repub), 
          main = "Votos al candidato presidencial republicano",
          srtCol = 60,
          dendrogram = "row",
          Rowv = dend,
          Colv = "NA", # this to make sure the columns are not ordered
          trace="none",          
          margins =c(3,6),      
          key.xlab = "% Votos",
          labCol = votes$Year,
          denscol = "grey",
          density.info = "density"
         )

```

Teniendo en cuenta el siguiente gráfico de correlaciones entre dendogramas.
```{r, eval=FALSE}
#install.packages('corrplot')
hclust_methods <- c("ward.D", "single", "complete", "average")
votes.repub_dendlist <- dendlist()

for(i in seq_along(hclust_methods)) {
   tmp_dend <- votes.repub %>% asinTransform %>% dist %>% hclust(method = hclust_methods[i]) %>% as.dendrogram 
   votes.repub_dendlist <- dendlist(votes.repub_dendlist, tmp_dend)
}
names(votes.repub_dendlist) <- hclust_methods

corrplot::corrplot(cor.dendlist(votes.repub_dendlist), "pie", "full")
```

representamos el tanglegram entre el completo y el método que menos se le parezca.
```{r, eval=FALSE} 

dend_com <- color_branches(as.dendrogram(hclust(d_votes, method = "complete")), k=2)
dend_com <- rotate(dend_com, labels(dend))
dend_ward <- color_branches(as.dendrogram(hclust(d_votes, method = "ward.D")), k=2)
dend_ward <- rotate(dend_ward, labels(dend))

dends <- dendlist(complete = dend_com, ward = dend_ward)
tanglegram(dends, margin_inner = 6)
```

Ahora repetimos lo mismo pero con el método más distinto al completo.

```{r , eval=FALSE}

dend_com <- color_branches(as.dendrogram(hclust(d_votes, method = "complete")), k=2)
dend_com <- rotate(dend_com, labels(dend))
dend_sing <- color_branches(as.dendrogram(hclust(d_votes, method = "single")), k=2)
dend_sing <- rotate(dend_sing, labels(dend))

dends <- dendlist(complete = dend_com, single = dend_sing)
tanglegram(dends, margin_inner = 6)
```

## Mixtura de Gaussianas

En esta parte ajustamos una mixtura de Gaussianas con la librería mixtools
y el dataset del geiser Old Faithful.

```{r , eval=FALSE}
#install.packages('mixtools')
library(mixtools)
data(faithful)
attach(faithful)
fix(faithful)
```

Representamos la distribución de los tiempos de espera entre erupciones
¿Cuántas modas se aprecian?

```{r, eval=FALSE} 
hist(waiting, main="Tiempo entre erupciones del Old Faithful",
     xlab="Minutos", ylab="", cex.main=1.5, cex.lab=1.5, cex.axis=1.4)
```
 Usando el método normalmixEM ajustamos dos MoG con 2 componentes.
Especificamos un número máximo de iteraciones de 30, y pesos iniciales iguales para cada componente.

```{r , eval=FALSE}
wait2 <- normalmixEM(waiting, lambda = 1, k=2, maxit = 30)
```

Comparamos en términos de log-verosimilitud. 

```{r , eval=FALSE}
print(wait2$loglik)
```

Usamos plot para observar el ajuste del MoG:

```{r, eval=FALSE} 
plot(wait2, density=TRUE, cex.axis=1.4, cex.lab=1.4, cex.main=1.8, main2="Time between Old Faithful eruptions", xlab2="Minutes")

```


