---
title: "MCIBLab2. Conceptos modernos en regresión lineal"
author: "DataLab CSIC"
date: " "
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducción

En este lab revisamos conceptos de selección de variables en 
regresión lineal y hacemos después regresión bayesiana.
La primera parte adapta el lab de ISLR ch 6. La parte bayesiana
adapta un lab de Jim Albert (detalles más abajo).

# Selección del mejor subconjunto hacia delante

Usamos la librería ISLR y sus datos Hitters que recogen info
sobre bateadores de beisbol. Cogemos algo de info.

```{r,eval=FALSE,echo=TRUE }
library(ISLR)
help(Hitters)
fix(Hitters)
names(Hitters)
dim(Hitters)
```
Vemos si hay casos con NA y los quitamos.
```{r,eval=FALSE,echo=TRUE }
sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```
Para la selección usamos la librería leaps. Hacemos help(leaps) 
para entenderla algo mejor. Luego help(regsubsets) con el mismo 
propósito.

En resultados * indica que se incluye la variable.
Desplegamos los estadísticos de selección y los resultados de 
BIC, como ejemplo. Ponemos después
los coeficientes de los seis primeros modelos (lo podemos hacer con
un bucle, pero así puedo discutirlo con más tranquilidad en clase)
para ver qué variables van entrando y cómo van cambiando los coeficientes.

```{r,eval=FALSE,echo=TRUE }
library(leaps)
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
reg.summary=summary(regfit.fwd)
names(reg.summary)
reg.summary$bic
summary(regfit.fwd)
plot(reg.summary$bic)
coef(regfit.fwd,1)
coef(regfit.fwd,2)
coef(regfit.fwd,3)
coef(regfit.fwd,4)
coef(regfit.fwd,5)
coef(regfit.fwd,6)
plot(regfit.fwd,scale="bic")
```

# Selección de modelos con conjunto de validación

Hacemos ahora selección de modelos primero con conjunto de validación.
Generamos primero el conjunto de validación.

```{r,eval=FALSE,echo=TRUE }
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
train
test=(!train)
test
```
Ahora encontramos el mejor subconjunto de variables.
Basados en el subconjunto de entrenamiento (train)
lo evaluamos en el conjunto de validación, en la estructura
model.matrix (crea la matrix X de diseño en modelo de regresión).
Inicializamos la estructura de datos val.errors donde se meterán
los errores de validación.
```{r,eval=FALSE,echo=TRUE }
regfit.best=regsubsets(Salary~.,data=Hitters[train,],nvmax=19)
test.mat=model.matrix(Salary~.,data=Hitters[test,])
val.errors=rep(NA,19)
val.errors
```
Calculamos los errores de validación (de una forma 
un poco engorrosa debido a regsubsets). Luego identificamos
el de menor error de validación y lo desplegamos.
```{r,eval=FALSE,echo=TRUE }
for(i in 1:19){
   coefi=coef(regfit.best,id=i)
   pred=test.mat[,names(coefi)]%*%coefi
   val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
val.errors
plot(val.errors)
which.min(val.errors)
coef(regfit.best,which.min(val.errors))
```
Para facilitar lo que viene después, se hace una función.
Así vemos cómo se pica una función (que creo no habíamos visto).
La parte más exótica es cómo se saca la fórmula
de la llamada a regsubsets. 
```{r,eval=FALSE,echo=TRUE }
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
```
La emplearemos ahora para hacer validación cruzada.

Ahora encontramos el mejor modelo por validación cruzada
con k=10 pliegues. set.seed fija la semilla. folds crea
los pliegues. cv.errors crea la estructura de datos donde
guardar resultados.
```{r,eval=FALSE,echo=TRUE }
k=10
set.seed(1)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
folds
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))
cv.errors
```
En el siguiente  bucle, j se refiere al pliegue j
e i al número de variables en el modelo. Luego usamos apply
para calcular el error medio. Usamos apply para calcular las medias.
```{r,eval=FALSE,echo=TRUE }
for(j in 1:k){
  best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)
  for(i in 1:19){
    pred=predict(best.fit,Hitters[folds==j,],id=i)
    cv.errors[j,i]=mean( (Hitters$Salary[folds==j]-pred)^2)
    }
  }
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
```
Finalmente dibujamos los resultados que sugieren que el mejor 
según CV-10 y lo desplegamos.
```{r,eval=FALSE,echo=TRUE }
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
which.min(mean.cv.errors)
reg.best=regsubsets(Salary~.,data=Hitters, nvmax=19)
coef(reg.best,which.min(mean.cv.errors))
```

# Ridge Regression 

Adaptamos la práctica de Ridge regression de ISLR. Seguimos
con los datos Hitters. Si empiezas aquí has de cargar ISLR
y hemos quitado los missing (o bien los has imputado de alguna forma).
Usaremos glmnet (que permite ridge, lasso y otras), pero tiene
una sintaxis de definición de modelos algo diferente. 
En particular hay que pasar la matriz de diseo y el vector de
respuestas, como hacemos aquí.

```{r,eval=FALSE,echo=TRUE }
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
```
Cargamos glmnet. Tiene un parámetro alpha que define el regularizador.
Con 0 es ridge. Definimos un grid de lambda de 10^-2 a 10^10. Fijate en sintaxis 
de glmnet.
```{r,eval=FALSE,echo=TRUE }
library(glmnet)
grid=10^seq(10,-2,length=100)
grid
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
```
Representamos los coeficientes para dos valores de lambda. El 50
es mayor que el 60 y fijate que pasa con la norma l2 de los coeficientes.
```{r,eval=FALSE,echo=TRUE }
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```
Partimos ahora en entrenamiento-test para comparar (luego) con lasso.
```{r,eval=FALSE,echo=TRUE }
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
```
Hacemos ridge regression en el conjunto de entrenamiento y evaluamos
en el conjunto test con lambda=4. Luego lo hacemos con un lambda
muy grande (equivalente a un modelo con solo intercept) y parece 
mejor el primer caso. Lo comparamos también con regresión estandar 
(equivalente a  ridge con lambda=0)
```{r,eval=FALSE,echo=TRUE }
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod,s=0,exact=T,type="coefficients",x=x[train,],y=y[train])[1:20,]
```
En general el 'mejor lambda' lo podemos obtner por
validación cruzada. Lo hacemos con la función de cv en glmnet 
denominada cv.glmnet (por defecto usa k=10 pliegues).
Hacemos después el MSE de test con ese mejor lambda
```{r,eval=FALSE,echo=TRUE }
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
```
Finalmente desplegamos los coeficientes para ese mejor modelo ridge, tras
reentrenarlo con todos los datos.
```{r,eval=FALSE,echo=TRUE }
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
```
No conseguimos hacer muchos coeficientes a 0.
# Lasso
Ahora aplicamos lasso. Usamos glmnet con alpha=1. Como antes. El plot
muestra como dependiendo de lambda algunos coeficientes son 0.
```{r,eval=FALSE,echo=TRUE }
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
```
Hacemos VC y evaluamos.
```{r,eval=FALSE,echo=TRUE }
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
```
Resultados parecidos a ridge y mejor que regresión estándar.
Y además hay unos cuantos coefs iguales a 0.
```{r,eval=FALSE,echo=TRUE }
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
```

# Regresión con componentes principales
Usamos PCR con la librería pls. Requiere tipificar los datos
(para PCR).
Además permite incluir el tipo de validación. Carga pls y haz help. 
Ojo informa de RMSE.
```{r,eval=FALSE,echo=TRUE }
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~., data=Hitters,scale=TRUE,validation="CV")
summary(pcr.fit)
```
Dibujamos los términos de validación. 16 parece una buena solución (pero son demasiadas
componentes, hay poca reducción).
```{r,eval=FALSE,echo=TRUE }
validationplot(pcr.fit,val.type="MSEP")
```
Usamos ahora validacion cruzada sobre el conjutno de entrenamiento.
```{r,eval=FALSE,echo=TRUE }
set.seed(1)
pcr.fit=pcr(Salary~., data=Hitters,subset=train,scale=TRUE, validation="CV")
validationplot(pcr.fit,val.type="MSEP")
```
Evaluamos sobre el conjunto test. Da resultados comparables a ridge o lasso
pero es más difícil de interpretar. Finalmente re-entrenamos con todo el 
conjunto.
```{r,eval=FALSE,echo=TRUE }
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
```

# Partial Least Squares
Finamente hacemos PLS con la función plsr del 
paquete pls. Misma historia: hacemos pls, mejor
solución, test error con 2 iteraciones y reentreno con 
esa mejor solución (que sale peor que ridge, lasso y pcr)
```{r,eval=FALSE,echo=TRUE }
set.seed(1)
pls.fit=plsr(Salary~., data=Hitters,subset=train,scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)
pls.fit=plsr(Salary~., data=Hitters,scale=TRUE,ncomp=2)
summary(pls.fit)
```

## Regresión lineal

Este ejemplo está adaptado del libro de Albert (Bayesian 
computation with R) y emplea el paquete LearnBayes y el conjunto de datos bidextinct. 
Para cada especie de pájaro se toman las variables TIME, tiempo medio de extinción; NESTING, número medio de pares que anidan; SIZE, tamaño de la especie (large o small); STATUS, status migratorio de la especie (migratorio o residente). Se desea ajustar un modelo que describa la variación en el tiempo hasta extinción de la especie en función de NESTING, SIZE y STATUS.  Cargamos el paquete LearnBayes, leemos el fichero y hacemos alguna exploración gráfica inicial. Hacemos la transformación log sobre la variable de interés TIME por
ser muy asimétrica hacia la derecha. Cruzamos log(time) con las otras variables.
Dos de ellas son discretas por lo que se perturban
```{r,eval=FALSE,echo=TRUE }
install.packages("LearnBayes")
library(LearnBayes)
data("birdextinct")
attach(birdextinct) 
head(birdextinct)
hist(time)
logtime=log(time) 
plot(nesting,logtime)  
out = (logtime > 3) 
text(nesting[out], logtime[out], label=species[out], pos = 2) 
plot(size,logtime)
plot(jitter(size),logtime,xaxp=c(0,1,1)) 
plot(jitter(status),logtime,xaxp=c(0,1,1))
```
El modelo que se formula es   E(log TIMEi|x, θ) = β0 + β1 NESTINGi + β2 SIZEi + β3 STATUSi.  Dos de las variables explicativas son categóricas con dos niveles y se representan como indicadores binarios. SIZE se codifica  0 (1) para pequeño (grande) y STATUS 0 (1) para migrante (residente). Hacemos primero el ajuste por mínimos cuadrados con el comando lm e interpretamos los resultados. Si no recuerdas lm, haz antes un help(lm).
```{r,eval=FALSE,echo=TRUE }
fit=lm(logtime~nesting+size+status,data=birdextinct,x=TRUE,y=TRUE)
summary(fit) 
```
Usamos ahora la función blinreg para muestrear de la a posteriori de ( β, σ), con una a priori no informativa. Sus entradas son el vector de respuestas, la matriz de diseño X y el tamaño muestral de la simulación.  Después mostramos los histogramas de los cuatro parámetros. Finalmente, resumimos las distribuciones mediantes los percentiles 5, 50, 95  de los datos simulados y comentamos.
```{r,eval=FALSE,echo=TRUE }
theta.sample=blinreg(fit$y,fit$x,5000)  
par(mfrow=c(2,2)) 
hist(theta.sample$beta[,2],main="NESTING",   xlab=expression(beta[1]))
hist(theta.sample$beta[,3],main="SIZE",  xlab=expression(beta[2])) 
hist(theta.sample$beta[,4],main="STATUS",   xlab=expression(beta[3])) 
hist(theta.sample$sigma,main="ERROR SD",  xlab=expression(sigma))  
apply(theta.sample$beta,2,quantile,c(.05,.5,.95)) 
quantile(theta.sample$sigma,c(.05,.5,.95))
```
Ahora empleamos la función blingrexpected para dar la respuesta esperada con cuatro casos nuevos referidos a combinaciones de SIZE y STATUS.
```{r,eval=FALSE,echo=TRUE }
cov1=c(1,4,0,0)
cov2=c(1,4,1,0)  
cov3=c(1,4,0,1)  
cov4=c(1,4,1,1)  
X1=rbind(cov1,cov2,cov3,cov4)  
mean.draws=blinregexpected(X1,theta.sample) 
c.labels=c("A","B","C","D")  
par(mfrow=c(2,2)) 
for (j in 1:4)  
   hist(mean.draws[,j],  main=paste("Covariate set",c.labels[j]),xlab="log TIME")
```
Ahora usamos blinregpred para hacer predicciones con los mismos casos anteriores. Comentamos los resultados comparando con el anterior
```{r,eval=FALSE,echo=TRUE }
X1=rbind(cov1,cov2,cov3,cov4) 
pred.draws=blinregpred(X1,theta.sample)  
c.labels=c("A","B","C","D")  
par(mfrow=c(2,2)) 
for (j in 1:4)   
      hist(pred.draws[,j],  main=paste("Covariate set",c.labels[j]),xlab="log TIME") 
```
Vemos ahora un método para evaluar (el ajuste d)el modelo. Una primera posibilidad es ver si las predicciones que hacemos cubren las observaciones. Lo hacemos con intervalos de probabilidad predictiva 0.9.  Los puntos que quedan fuera son posibles outliers. En el ejemplo vemos 3 (snipe, raven, and skylark).
```{r,eval=FALSE,echo=TRUE }
pred.draws=blinregpred(fit$x,theta.sample)
pred.sum=apply(pred.draws,2,quantile,c(.05,.95))
par(mfrow=c(1,1)) 
ind=1:length(logtime)  
matplot(rbind(ind,ind),pred.sum,type="l",lty=1,col=1,xlab="INDEX",ylab="log TIME") 
points(ind,logtime,pch=19) 
out=(logtime>pred.sum[2,])  
text(ind[out], logtime[out], label=species[out], pos = 4) 
```






