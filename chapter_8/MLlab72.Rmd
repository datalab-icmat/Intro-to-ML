---
title: "MLlab-7-2. Aprendizaje profundo para visión.
Intro"
author: "DataLab CSIC"
date: " "
output: word_document
---


# Introducción

En este lab realizamos varios ejercicios en relación con 
con aprendizaje profundo y visión por ordenador
con ayuda de Keras. Hacemos una versión en PC. 
Completaremos el último ejercicio con Google Collab.


# Recordatorio

Recuerda que hicimos ya un primer ejemplo con Keras.
Por si no lo hiciste, recuerda que has de cargarlo.

Copiamos lo que dijimos entonces. Echamos un ojito en  <https://blog.rstudio.com/2017/09/05/keras-for-r/> y 
<https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/>. Más breve con 
help(Keras). Esencialmente
tenemos que instalarla el paquete Keras R, cargar el núcleo de keras y luego
con install_keras() cargamos también TensorFlow (esto se hace una vez, reinicializa
RStudio cuando lo hayas hecho lo comentas)
```{r, eval=FALSE}
library(keras)
help(keras)
#install_keras()
```


## Redes profundas para visión


En este ejercicio construimos una red profunda completamente conectada 
para reconocer los datos MNIST y la comparamos con un random Forest. En
la siguiente parte se emplea una CNN. Esta parte adapta un ejercicio 
de Alberto Torres, Victor Gallego y Roi Naveiro.

Cargamos los paquetes necesarios. Si no los has instalado previamente,
habrás de hacerlo. ggplot2 no es necesaria pero salen mejores gráficos.
```{r, eval=FALSE}
library(randomForest)
library(keras)
library(ggplot2)
```

Introducimos primero una función auxiliar que hace 
una gráfica de la imagen en cuestión. Usa las funciones
t, apply, rev e image!!!! En su caso haz help. Usamos también plot
un poco más abajo cuando mostremos imágenes.

```{r, eval=FALSE}
show_digit = function(img){
  img = t( apply(img, 2, rev) )
  image( img )
}
```

Cargamos los datos MNIST de train y test en memoria.
Vienen por defecto al cargar keras. Hacemos help para coger
algo de información. 
Las imágenes son tensores 3D y las etiquetas 1D, en correspondencia 1-1.
str ayuda a desplegar la estructura de objetos. También usamos algunos
comandos para entender los tensores involucrados.
Finalmente, visualizamos un par de ejemplos.

```{r, eval=FALSE}
help("dataset_mnist")
help(str)

mnist <- dataset_mnist()
x_train <- mnist$train$x
# Aquí van algunos comandos para averiguar el contenido
str(x_train)
length(dim(x_train))
dim(x_train)
typeof(x_train)
# esto sirve si queremos solo algunas de las imágenes
# fiajos que lo podeís asimilar a un batch de 100 ejemplos
misimagenes<-x_train[10:99,,]
dim(misimagenes)
# esto sirve si queremos solo algunos trozos de algunas imágenes
misimagenes<-x_train[10:99,15:28,15:28]
dim(misimagenes)
# retomamos el hilo 
y_train <- mnist$train$y
str(y_train)
# Ahora viene el conjunto de test
x_test <- mnist$test$x
y_test <- mnist$test$y
# mostramos un par de ejmplos con plot y show digit
y_train[5]
plot(as.raster(x_train[5,,],max=255))
show_digit(x_train[5,,])
x_train[5,,]
x_train[5,,]/255
y_train[15]
plot(as.raster(x_train[15,,],max=255))
show_digit(x_train[15,,])
```

Antes del entrenamiento, necesitamos aplanar los datos,
esto es, convertimos los tensores  28*28=784 en vectores de 
dimensión 784. Además, debemos pasar las etiquetas a
notación oHE (one-hot-encoding). Haced help de array-reshape
en la parte de tensorflow (ojo a la discusión por filas y 
por columnas associado a dim)
```{r, eval=FALSE}
help("array_reshape")
# ESTO SERIA CON dim
#dim(x_train) <- c(nrow(x_train), 784)
# PERO HAY QUE HACERLO CON array_reshape
x_train<-array_reshape(x_train,c(60000,28*28))
# estos tres es para que veais el efecto, comparandolo con el de más arriba
length(dim(x_train))
dim(x_train)
typeof(x_train)
str(x_train)
# retomamos el hilo aqui
x_test<-array_reshape(x_test,c(10000,28*28)) 
y_train <- to_categorical(y_train, 10)
str(y_train)
head(y_train)
y_test <- to_categorical(y_test, 10)
```

Empezamos definiendo un modelo de regresión logística con 
10 clases. Desplegamos el resumen del modelo para aprehenderlo;
observa, en particular, el número de parámetros entrenables.
Recordamos antes con help lo que es layer_dense

```{r, eval=FALSE}
help(layer_dense)
model_lr <- keras_model_sequential() 
model_lr %>% 
  layer_dense(units = 10, input_shape = c(784), activation = "softmax")
summary(model_lr)
```
Ahora definimos la entropía cruzada como función de coste y rmsprop como optimizador. Entrenamos con 30 épocas, minilotes de 128 y 20% del conjunto 
para validación.
```{r, eval =FALSE}
help(compile)
help(fit)
model_lr %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy")
)
history <- model_lr %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```
Al observar la traza, parece que se estanca en el 
aprendizaje, por un lado. Además, segun early stopping 
podríamos parar en la epoch correspondiente por subir la pérdida  de evaluación.
El accuracy que se alcanza está alrededor del 90%.
Para mejorar reescalamos a [0,1] y repetimos.
```{r, eval=FALSE}
x_train <- x_train / 255
x_test <- x_test / 255
history <- model_lr %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)

plot(history)

model_lr %>% evaluate(x_test, y_test,verbose = 0)
```
Alcanzamos exactitud (accuracy) de 93%.

Mejoramos el aprendizaje con  una red profunda.
La arquitectura pasa el input por una capa densa con 256 unidades ocultas con activación tipo relu. La salida de estas capas pasa a otra capa densa con 128 unidades, también con activación relu. Finalmente, esta capa mandará señal a la capa final con 10 unidades y activación softmax para así recuperar probabilidades. Incluye regularización L2 (weight decay, ridge regression) en las dos primeras capas, con parámetro 0.001. Resumimos luego el modelo; fíjate en el número de
parámetros.

```{r, eval=FALSE}
model_l2 <- keras_model_sequential() 
model_l2 %>% 
  layer_dense(units = 256, activation = "relu", input_shape = c(784), kernel_regularizer = regularizer_l2(l = 0.001) ) %>% 
  layer_dense(units = 128, activation = "relu", kernel_regularizer = regularizer_l2(l = 0.001) ) %>%
  layer_dense(units = 10, activation = "softmax")
summary(model_l2)
```

Una vez definido el modelo lo compilamos con 
entropía cruzada como función de coste y rmsprop como optimizador.
Luego entrena el modelo con 30 épocas, minilotes de 128
y el 20% del conjunto de entrenamiento para validación, como antes.
Finalmente, hacemos la evaluación en el conjunto test.
```{r,eval=FALSE}
model_l2 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy")
)
history <- model_l2 %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
# este plot despliega el gráfico anterior de nuevo, un poco
# más pro.
plot(history)
# evaluamos en el conjunto test
model_l2 %>% evaluate(x_test, y_test,verbose = 0)
```
Alcanzamos 97% acc aunque hay algo de overfit (a partir de 
cierta iteración). 
6). 

Ahora entrenamos la misma red que en el caso anterior. Sin embargo,
en lugar de regularización L2, empleamos dropout con proporción 0.4 en la primera capa y 0.3 en la segunda.  Hacemos help de este nuevo tipo de capa
y ya ponemos el resto seguido pues es igual. Visualiza la estructura:
definición, compilación, entrenamiento (y evaluación el conjunto test)
```{r, eval=FALSE}
help(layer_dropout)

model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = "relu", input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = "softmax")
summary(model)

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy")
)


history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)

plot(history)

model %>% evaluate(x_test, y_test,verbose = 0)
```
Hemos mejorado algo aunque hay algún signo de sobreajuste.

Finalmente, concluimos con algunos acciones de mantenimiento del
modelo. Primero lo guardamos en un fichero 
denominado "mnist_weights.hdf5". 
Busca wikipedia hdf5.

Después carga el modelo de nuevo y realiza predicciones sobre el conjunto de test.

```{r, eval=FALSE}
help("save_model_hdf5")
help("load_model_hdf5")

save_model_hdf5(model, filepath = "mnist_weights.hdf5", overwrite = TRUE,
  include_optimizer = TRUE)

new_model = load_model_hdf5("mnist_weights.hdf5", custom_objects = NULL, compile = TRUE)

new_model %>% predict_classes(x_test)
```

Como comparación final, empleamos otro de los modelos vistos antes,
un Random Forest. Primero pasamos las matrices de train y test a dataframe.

```{r, eval=FALSE}
train = data.frame(x_train)
train$y = as.factor(mnist$train$y)

test = data.frame(x_test)
test$y = as.factor(mnist$test$y)
```

Ahora entrenamos el modelo. Usaremos solo 500 ejemplos de entrenamiento. La implementación de Random Forest de R no permite minibatches, con lo que usar todo el conjunto de entrenamiento sería muy costoso computacionalmente.

```{r}
fit_rf = randomForest::randomForest(y ~ ., data = train[1:500, ])
fit_rf$confusion
test_pred = predict(fit_rf, test)
mean(test_pred == test$y)
table(predicted = test_pred, actual = test$y)
```

Comenta, para concluir como se comporta RF respecto a DNN.

## Ahora con una red convolutiva

En esta parte, repetimos el análisis con una red convolutiva.
Cargamos keras y definimos la función auxiliar.
Cargamos los datos. 
Todo como antes, pero lo ponemos otra vez por si estáis empezando por aquí!!!
Hay alguna pequeña variación.
```{r, eval=FALSE}
library(keras)

show_digit = function(img){
  img = t( apply(img, 2, rev) )
  image( img )
}

img_rows <- 28
img_cols <- 28

mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```

De nuevo aplanamos con reshape y escalamos. Y convertimos las etiquetas como antes.


```{r}
help("array_reshape")
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)

x_train <- x_train / 255
x_test <- x_test / 255

cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')

num_classes <- 10
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
```

Pasamos ya a definir la red convolutiva profunda con estas 
características. 

Defenimos una red convolutiva:

  1. Capa convolutiva de 32 filtros, kernel $3\times 3$ y activaciones ReLU
  2. Capa convolutiva de 64 filtros, kernel $3\times 3$ y activaciones ReLU
  3. Max pooling de tamaño $2 \times 2$
  4. Dropout ($0.25$)
  5. Capa densa de 128 unidades y activación ReLU
  6. Dropout ($0.5$)
  7. Capa de salida

Previa a la definición miramos los nuevos comandados.
```{r, eval = FALSE}
help("layer_conv_2d")
help("layer_max_pooling_2d")
help("layer_flatten")
help("layer_dropout")

modelf <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
                input_shape = input_shape) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = num_classes, activation = 'softmax')
summary(model)
```

Compilamos el modelo con Adam

```{r, eval = FALSE}
modelf %>% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)
```

Entrenamos modelo durante 2 épocas, con tamaño de minibatch 128 y 20%
de split para validación. Ponemos solo 2 épocas por ser muy lento
en nuestro PC. 
En Google collab pondremos más épocas.
```{r, eval = FALSE}
batch_size <- 128
epochs <- 2

history<-modelf %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = 0.2
)


plot(history)

modelf %>% evaluate(x_test, y_test,verbose = 0)
```

