---
title: "LAB 1.3 ML"
author: "DataLab CSIC"
date: "2/2/2023"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introducción

En este lab hacemos una intro a ML.  Recordamos regresión líneal y el problema de sobreajuste. Después se discute el problema de selección de modelos en knns y concluimos con una breve ilustración de regresión logística.  Revisamos como ejemplo de aprendizaje no supervisado algunas cuestiones sobre ACPs, primero una ilustración básica y luego dos ilustraciones a proyección y compresión. Finalmente, se hace un
revisión de optimización y validación en R.

En esta tercera parte ilustramos algunos usos de PCA. Primero un ejemplo clásico con los datos USAarrests  y después algunos ejemplos de imágenes para que reflexionemos sobre el impacto de la dimensionalidad. 

## PCA. Caso básico

Usamos los datos USAarrests. Cogemos primero algo de info sobre los mismos. 
```{r ,eval=FALSE}
help(USArrests)
dim(USArrests)
USArrests[1:10,]
summary(USArrests)
plot(USArrests)
```
Hacemos PCA sobre tales datos. Primero haced el help para recordar sintaxis y métodos. Después visualizamos la importancia de las componentes
```{r ,eval=FALSE}
help(prcomp)
prcomp(USArrests)
plot(prcomp(USArrests))
summary(prcomp(USArrests))
```
Como sabéis PCA es bastante sensible a datos no tipificados. Intentamos dar una interpretación semántica a las componentes
```{r ,eval=FALSE}
plot(prcomp(USArrests,scale=T))
summary(prcomp(USArrests,scale=T))
prcomp(USArrests,scale=T)
```
Dibujamos las dos primeras componentes y añadimos nombres 
```{r ,eval=FALSE}
plot(prcomp(USArrests,scale=T)$x[,1:2])
plot(prcomp(USArrests,scale=T)$x[,1:2],type="n")
text(prcomp(USArrests,scale=T)$x[,1:2],rownames(USArrests))
``` 
Podemos mejorar gráfico con biplot. Primero échale un ojo a https://es.wikipedia.org/wiki/Biplot   Después haz help(biplot)
```{r ,eval=FALSE}
biplot(prcomp(USArrests,scale=T)) 
``` 

## PCA proyección

En este ejemplo mostramos el uso de PCA para proyectar datos de alta dimensión (imágenes) a unas pocas dimensiones. Usaremos el conjunto MINIST. Adaptamos aquí el lab preparado por Victor Gallego y Roi Naveiro, de ICMAT.  Definimos primero tres funciones que emplearemos
* show_digit: Hace una gráfica del dígito en cuestión.
```{r ,eval=FALSE}
show_digit = function(arr784, col = gray(12:1 / 12), ...) {
  image(matrix(as.matrix(arr784[-785]), nrow = 28)[, 28:1], col = col, ...)
}
``` 
* load_image_file: Para cargar las imágenes de los dígitos
```{r ,eval=FALSE}
load_image_file = function(filename) {
  ret = list()
  f = file(filename, 'rb')
  readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
  close(f)
  data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
}
``` 
* load_label_file: Para cargar las etiquetas
```{r ,eval=FALSE}
load_label_file = function(filename) {
  f = file(filename, 'rb')
  readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
  close(f)
  y
}
```
Hacemos la lectura de datos cargando el célebre conjunto MNIST. Ojo al path. Esta base de datos consta de 10000 imágenes en escala de gris a 28 x 28, de los dígitos del 0 al 9 (escritos a mano). Tras cargarlo, vemos sus dimensiones y desplegamos algunos ejemplos.
```{r ,eval=FALSE}
test  = load_image_file("C:/Users/David/Desktop/clases/cursoML/labs/lab1/t10k-images.idx3-ubyte")
test$y  = as.factor(load_label_file("C:/Users/David/Desktop/clases/cursoML/labs/lab1/t10k-labels.idx1-ubyte"))
dim(test)
show_digit(test[1, ])
show_digit(test[100, ])
show_digit(test[500, ])
```
Ahora hacemos una  proyección a 2D usando PCA empleando el paquete prcomp y representamos las dos primeras componentes. Quitamos LABEL 'para no ahcer trampas'
```{r ,eval=FALSE}
proy_pca <- prcomp(test[, 1:28^2], retx = T) 
plot(proy_pca$x[, 1:2], type = 'n')
text(proy_pca$x[, 1:2], labels = test$y, cex = 0.5,
     col = rainbow(length(levels(test$y)))[test$y])
```
Dibujamos la curva del número de componentes frente a proporción de varianza explicada. Vemos cuantas componentes son necesarias para explicar el 99% de la varianza. Instala en su caso ggplot2
```{r ,eval=FALSE}
install.packages("ggplot2")
library(ggplot2)
eigs = summary(proy_pca)$sdev^2
max_ncom = dim(test)[2]-1
df = data.frame( 1:max_ncom, eigs, eigs/sum(eigs), cumsum(eigs)/sum(eigs))
colnames(df) = c("N_comp", "Eigenvalues", "ExpVar", "CumuExpVar")
p = ggplot(data=df) + geom_line(aes(x = N_comp, y = CumuExpVar) )
p
```

## PCA compresión

Esta parte adapta una práctica de Victor Gallego y Roi Naveiro. Se aplica PCA para comprimir imágenes de caras. Esto tiene numerosos usos como veremos más adelante. Para empezar, tenemos que descargar los datos de 

 https://drive.google.com/file/d/14f9gJ3SrT2zE8iokixzZBOalvZ4U7YGg/view?usp=sharing
 
Los descomprimis. Os crea un directorio thumbnails128x128 en el path. Echad un ojo a su contenido.
Poneis la librería EBImage del proyecto BiocManager y luego cargais, ojo a la ruta
```{r, eval=F}
install.packages("BiocManager") 
BiocManager::install("EBImage")
library(EBImage)
faces_files <- list.files(path = "/Users/David/Desktop/clases/cursoML/labs/lab1/thumbnails128x128", pattern = ".png",                      all.files = TRUE,full.names = TRUE,no.. = TRUE)
faces <- readImage(faces_files)
```
Se representan algunas imágenes
```{r, eval=F}
plot(faces[,,,10])
plot(faces[,,,20])
```
Aplicamos ahora PCA con prcomp con el propósito de comprimir. Ojo tarda un poquito
bastante. Ponemos 4466 caras
```{r, eval=F}
faces_flat <- t(array(faces, dim=c(128^2*3, 4466)))
proy_faces <- prcomp(faces_flat[, 1:(128^2*3)], center = FALSE)
```
Ahora encontramos el número de componentes principales necesarias para que el error de reconstrucción sea como mucho del 1%.
```{r, eval=F}
eigvals <- proy_faces$sdev^2
ratio <- eigvals / sum(eigvals)
ratio_acum <- cumsum(ratio)
M = which(ratio_acum >= 0.99)[1]
```
Y hacemos finalmente la reconstrucción, por ejemplo de la imagen 10 anterior
```{r, eval=F}
faces_recons_flat <- proy_faces$x[,1:M] %*% t(proy_faces$rotation[,1:M])
faces_recons_flat[faces_recons_flat<0] <- 0
faces_recons_flat[faces_recons_flat>1] <- 1
faces_recons <- array(t(faces_recons_flat), dim=dim(faces))
fs = Image(faces_recons, colormode = 'Color')
plot((fs[,,,10]))
```

